"""
Tool 2: ì›¹ ê²€ìƒ‰ ì—”ì§„ (ë„¤ì´ë²„ API ì‚¬ìš©)
Web Search Engine for Latest Plant Information using Naver API
"""

import requests
import json
from typing import List, Dict, Optional
import urllib.parse
import os
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebSearchTool:
    """ë„¤ì´ë²„ ê²€ìƒ‰ APIë¥¼ ì‚¬ìš©í•œ ì‹ë¬¼ ê´€ë ¨ ìµœì‹  ì •ë³´ ê²€ìƒ‰ ë„êµ¬"""
    
    def __init__(self):
        # ë„¤ì´ë²„ API í‚¤ ì„¤ì •
        self.client_id = "cIchWx3damfG74qMyJgv"
        self.client_secret = "jE1ZgoxNLg"
        
        self.headers = {
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret,
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ë„¤ì´ë²„ ê²€ìƒ‰ API ì—”ë“œí¬ì¸íŠ¸
        self.search_url = "https://openapi.naver.com/v1/search/blog.json"
        
        logger.info("ğŸ” WebSearchTool ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ğŸ“¡ ë„¤ì´ë²„ API ì—”ë“œí¬ì¸íŠ¸: {self.search_url}")
        
    def search_plant_info(self, query: str, max_results: int = 3) -> List[Dict]:
        """ë„¤ì´ë²„ ë¸”ë¡œê·¸ ê²€ìƒ‰ì„ í†µí•œ ì‹ë¬¼ ì •ë³´ ê²€ìƒ‰"""
        logger.info(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}' (ìµœëŒ€ {max_results}ê°œ ê²°ê³¼)")
        
        try:
            # ì‹ë¬¼ ê´€ë ¨ ê²€ìƒ‰ì–´ë¡œ ë³´ê°•
            enhanced_query = f"ì‹ë¬¼ {query} í‚¤ìš°ê¸° ê´€ë¦¬"
            logger.info(f"ğŸ”§ ë³´ê°•ëœ ê²€ìƒ‰ì–´: '{enhanced_query}'")
            
            # ë„¤ì´ë²„ ê²€ìƒ‰ API í˜¸ì¶œ
            params = {
                'query': enhanced_query,
                'display': max_results,
                'start': 1,
                'sort': 'sim'  # ì •í™•ë„ìˆœ ì •ë ¬
            }
            
            logger.info(f"ğŸ“¤ ë„¤ì´ë²„ API ìš”ì²­ ì‹œì‘...")
            logger.info(f"   - URL: {self.search_url}")
            logger.info(f"   - íŒŒë¼ë¯¸í„°: {params}")
            logger.info(f"   - í—¤ë”: X-Naver-Client-Id: {self.client_id[:10]}...")
            
            response = requests.get(
                self.search_url,
                headers=self.headers,
                params=params,
                timeout=10
            )
            
            logger.info(f"ğŸ“¥ ë„¤ì´ë²„ API ì‘ë‹µ ë°›ìŒ: HTTP {response.status_code}")
            
            if response.status_code == 200:
                data = response.json()
                total_results = data.get('total', 0)
                items = data.get('items', [])
                
                logger.info(f"âœ… ê²€ìƒ‰ ì„±ê³µ!")
                logger.info(f"   - ì „ì²´ ê²°ê³¼ ìˆ˜: {total_results}")
                logger.info(f"   - ë°˜í™˜ëœ í•­ëª© ìˆ˜: {len(items)}")
                
                # ê° ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                for i, item in enumerate(items[:3]):
                    title = self._clean_html_tags(item.get('title', ''))[:50]
                    logger.info(f"   - ê²°ê³¼ #{i+1}: {title}...")
                
                processed_results = self._process_naver_results(items)
                logger.info(f"ğŸ¯ ìµœì¢… ì²˜ë¦¬ëœ ê²°ê³¼: {len(processed_results)}ê°œ")
                return processed_results
                
            else:
                logger.error(f"âŒ ë„¤ì´ë²„ API ì˜¤ë¥˜: HTTP {response.status_code}")
                logger.error(f"   - ì‘ë‹µ ë‚´ìš©: {response.text[:200]}...")
                logger.info("ğŸ”„ Fallback ë°ì´í„° ì‚¬ìš©")
                return self._get_fallback_results(query)
                
        except Exception as e:
            logger.error(f"ğŸ’¥ ê²€ìƒ‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
            logger.info("ğŸ”„ Fallback ë°ì´í„° ì‚¬ìš©")
            return self._get_fallback_results(query)
    
    def _process_naver_results(self, items: List[Dict]) -> List[Dict]:
        """ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬"""
        logger.info(f"ğŸ”§ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬ ì‹œì‘: {len(items)}ê°œ í•­ëª©")
        
        results = []
        
        for i, item in enumerate(items):
            # HTML íƒœê·¸ ì œê±°
            title = self._clean_html_tags(item.get('title', ''))
            description = self._clean_html_tags(item.get('description', ''))
            
            result = {
                'title': title,
                'content': description,
                'source': item.get('bloggername', 'ë„¤ì´ë²„ ë¸”ë¡œê·¸'),
                'url': item.get('link', ''),
                'date': item.get('postdate', ''),
                'relevance_score': 0.9 - (i * 0.1)
            }
            
            results.append(result)
            
            logger.info(f"   - ì²˜ë¦¬ ì™„ë£Œ #{i+1}:")
            logger.info(f"     ì œëª©: {title[:50]}...")
            logger.info(f"     ë‚´ìš©: {description[:80]}...")
            logger.info(f"     ì¶œì²˜: {result['source']}")
            
        logger.info(f"âœ… ê²°ê³¼ ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ")
        return results
    
    def _clean_html_tags(self, text: str) -> str:
        """HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬"""
        import re
        
        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # HTML ì—”í‹°í‹° ë³€í™˜
        text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
        text = text.replace('&quot;', '"').replace('&#39;', "'")
        
        return text.strip()
    
    def _get_fallback_results(self, query: str) -> List[Dict]:
        """API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ê²°ê³¼"""
        logger.info(f"ğŸ›¡ï¸ Fallback ë°ì´í„° ìƒì„± ì¤‘: '{query}'")
        
        fallback_info = {
            'ëª¬ìŠ¤í…Œë¼': 'ëª¬ìŠ¤í…Œë¼ëŠ” ë°ì€ ê°„ì ‘ê´‘ì„ ì¢‹ì•„í•˜ê³ , í† ì–‘ì´ ë§ˆë¥´ë©´ ì¶©ë¶„íˆ ë¬¼ì„ ì£¼ì„¸ìš”. ìŠµë„ê°€ ë†’ì€ í™˜ê²½ì„ ì„ í˜¸í•´ìš”.',
            'ê³ ë¬´ë‚˜ë¬´': 'ê³ ë¬´ë‚˜ë¬´ëŠ” í‚¤ìš°ê¸° ì‰¬ìš´ ì‹ë¬¼ë¡œ, ë°ì€ ê³³ì—ì„œ ê¸°ë¥´ê³  ê²‰í™ì´ ë§ëì„ ë•Œ ë¬¼ì„ ì£¼ë©´ ë©ë‹ˆë‹¤.',
            'ì‚°ì„¸ë² ë¦¬ì•„': 'ì‚°ì„¸ë² ë¦¬ì•„ëŠ” ë¬¼ì„ ì ê²Œ ì¤˜ë„ ë˜ëŠ” ì‹ë¬¼ì´ì—ìš”. í•œ ë‹¬ì— 1-2ë²ˆ ì •ë„ë©´ ì¶©ë¶„í•´ìš”.',
            'ìŠ¤íˆ¬í‚¤': 'ìŠ¤íˆ¬í‚¤ëŠ” ê³µê¸°ì •í™” íš¨ê³¼ê°€ ë›°ì–´ë‚˜ê³ , ë¬¼ì„ ìì£¼ ì£¼ì§€ ì•Šì•„ë„ ì˜ ìë¼ëŠ” ì‹ë¬¼ì´ì—ìš”.'
        }
        
        # ì¿¼ë¦¬ì—ì„œ ì‹ë¬¼ ì´ë¦„ ì°¾ê¸°
        plant_info = "ì‹ë¬¼ì€ ì ì ˆí•œ ë¬¼ì£¼ê¸°ì™€ ë¹›ì´ ì¤‘ìš”í•´ìš”. ê° ì‹ë¬¼ì˜ íŠ¹ì„±ì— ë§ê²Œ ê´€ë¦¬í•´ì£¼ì„¸ìš”!"
        matched_plant = "ì¼ë°˜"
        
        for plant_name, info in fallback_info.items():
            if plant_name in query:
                plant_info = info
                matched_plant = plant_name
                break
        
        logger.info(f"   - ë§¤ì¹­ëœ ì‹ë¬¼: {matched_plant}")
        logger.info(f"   - ì œê³µí•  ì •ë³´: {plant_info[:50]}...")
        
        result = [{
            'title': f'{query} ê´€ë ¨ ì •ë³´',
            'content': plant_info,
            'source': 'ì‹ë¬¼ ê´€ë¦¬ ê°€ì´ë“œ',
            'url': 'https://plant-care-guide.com',
            'relevance_score': 0.7
        }]
        
        logger.info(f"ğŸ›¡ï¸ Fallback ê²°ê³¼ ìƒì„± ì™„ë£Œ: 1ê°œ")
        return result
    
    def search_trends(self, topic: str = "ì‹ë¬¼ íŠ¸ë Œë“œ") -> str:
        """ìµœì‹  ì‹ë¬¼ íŠ¸ë Œë“œ ê²€ìƒ‰"""
        logger.info(f"ğŸ“ˆ íŠ¸ë Œë“œ ê²€ìƒ‰: '{topic}'")
        
        results = self.search_plant_info(f"2024 ìµœì‹  {topic}")
        
        if results:
            trend_info = []
            for result in results:
                trend_info.append(f"â€¢ {result['content'][:100]}...")
            
            final_result = f"ğŸ” **ìµœì‹  {topic} ì •ë³´**:\n" + "\n".join(trend_info)
            logger.info(f"ğŸ“ˆ íŠ¸ë Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return final_result
        else:
            logger.warning(f"ğŸ“ˆ íŠ¸ë Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: '{topic}'")
            return f"ì£„ì†¡í•´ìš”, {topic} ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš” ğŸ˜…"
    
    def search_care_tips(self, plant_name: str) -> str:
        """íŠ¹ì • ì‹ë¬¼ ê´€ë¦¬ íŒ ê²€ìƒ‰"""
        logger.info(f"ğŸŒ¿ ê´€ë¦¬ íŒ ê²€ìƒ‰: '{plant_name}'")
        
        results = self.search_plant_info(f"{plant_name} ê´€ë¦¬ í‚¤ìš°ê¸°")
        
        if results:
            tips = []
            for result in results:
                tips.append(f"ğŸ“Œ {result['content'][:150]}...")
            
            final_result = f"ğŸŒ¿ **{plant_name} ê´€ë¦¬ íŒ**:\n" + "\n".join(tips)
            logger.info(f"ğŸŒ¿ ê´€ë¦¬ íŒ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return final_result
        else:
            logger.warning(f"ğŸŒ¿ ê´€ë¦¬ íŒ ê²€ìƒ‰ ì‹¤íŒ¨: '{plant_name}'")
            return f"{plant_name}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì¼ë°˜ì ì¸ ì‹ë¬¼ ê´€ë¦¬ ë°©ë²•ì„ ì•Œë ¤ë“œë¦´ê¹Œìš”? ğŸ¤”"
    
    def search_plant_problems(self, problem: str) -> str:
        """ì‹ë¬¼ ë¬¸ì œ í•´ê²° ì •ë³´ ê²€ìƒ‰"""
        logger.info(f"ğŸš¨ ë¬¸ì œ í•´ê²° ê²€ìƒ‰: '{problem}'")
        
        results = self.search_plant_info(f"ì‹ë¬¼ {problem} í•´ê²° ë°©ë²•")
        
        if results:
            solutions = []
            for result in results:
                solutions.append(f"ğŸ’¡ {result['content'][:120]}...")
            
            final_result = f"ğŸš¨ **{problem} í•´ê²° ë°©ë²•**:\n" + "\n".join(solutions)
            logger.info(f"ğŸš¨ ë¬¸ì œ í•´ê²° ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
            return final_result
        else:
            logger.warning(f"ğŸš¨ ë¬¸ì œ í•´ê²° ê²€ìƒ‰ ì‹¤íŒ¨: '{problem}'")
            return f"{problem}ì— ëŒ€í•œ í•´ê²°ì±…ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ì „ë¬¸ê°€ì—ê²Œ ë¬¸ì˜í•´ë³´ì‹œëŠ” ê²ƒì„ ì¶”ì²œí•´ìš”! ğŸŒ±"
    
    def get_web_summary(self, query: str) -> Dict:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        logger.info(f"ğŸ“Š ê²€ìƒ‰ ìš”ì•½ ìƒì„±: '{query}'")
        
        results = self.search_plant_info(query)
        
        if not results:
            logger.warning(f"ğŸ“Š ê²€ìƒ‰ ìš”ì•½ ì‹¤íŒ¨: ê²°ê³¼ ì—†ìŒ")
            return {
                'summary': 'ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ìš” ğŸ˜”',
                'sources': 0,
                'confidence': 0.0
            }
        
        # ê²°ê³¼ ìš”ì•½
        all_content = []
        sources = []
        
        for result in results:
            all_content.append(result['content'][:200])  # ê¸¸ì´ ì œí•œ
            sources.append(result['source'])
        
        summary = " ".join(all_content)
        confidence = sum(r.get('relevance_score', 0.5) for r in results) / len(results)
        
        summary_result = {
            'summary': summary,
            'sources': len(sources),
            'confidence': confidence,
            'source_list': sources
        }
        
        logger.info(f"ğŸ“Š ê²€ìƒ‰ ìš”ì•½ ì™„ë£Œ:")
        logger.info(f"   - ìš”ì•½ ê¸¸ì´: {len(summary)} ë¬¸ì")
        logger.info(f"   - ì¶œì²˜ ìˆ˜: {len(sources)}")
        logger.info(f"   - ì‹ ë¢°ë„: {confidence:.2f}")
        
        return summary_result


# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    print("=== ë„¤ì´ë²„ API ì›¹ ê²€ìƒ‰ ë„êµ¬ ë””ë²„ê¹… í…ŒìŠ¤íŠ¸ ===")
    
    search_tool = WebSearchTool()
    
    # íŠ¸ë Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("1. íŠ¸ë Œë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*50)
    trend_result = search_tool.search_trends("ì‹¤ë‚´ì‹ë¬¼ íŠ¸ë Œë“œ")
    print(f"ê²°ê³¼:\n{trend_result}")
    
    # ê´€ë¦¬ íŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("2. ê´€ë¦¬ íŒ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*50)
    care_result = search_tool.search_care_tips("ëª¬ìŠ¤í…Œë¼")
    print(f"ê²°ê³¼:\n{care_result}")
    
    # ë¬¸ì œ í•´ê²° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("3. ë¬¸ì œ í•´ê²° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*50)
    problem_result = search_tool.search_plant_problems("ìì´ ë…¸ë˜ì§")
    print(f"ê²°ê³¼:\n{problem_result}")
    
    # ì¼ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n" + "="*50)
    print("4. ì¼ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    print("="*50)
    general_result = search_tool.get_web_summary("ì‹ë¬¼ ë¬¼ì£¼ê¸°")
    print(f"ìš”ì•½: {general_result['summary'][:200]}...")
    print(f"ì‹ ë¢°ë„: {general_result['confidence']:.2f}")
    print(f"ì¶œì²˜: {general_result['source_list']}")